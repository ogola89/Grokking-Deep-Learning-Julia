{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_hidden (generic function with 2 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "import Base:+,-,*,println, sum, broadcasted, size, adjoint, show, dropdims, tanh\n",
    "using Base.Iterators:partition, flatten\n",
    "\n",
    "mutable struct Tensor\n",
    "    data\n",
    "    autograd\n",
    "    creators\n",
    "    creation_op\n",
    "    id\n",
    "    children\n",
    "    grad \n",
    "    index_select_indices\n",
    "    softmax_output\n",
    "    target_dist\n",
    "    \n",
    "    function Tensor(data; autograd=false, creators=nothing, creation_op = nothing, id=nothing)\n",
    "        if isnothing(id)\n",
    "            id = rand(1:100000)\n",
    "        end\n",
    "        T = new(data, autograd, creators, creation_op, id)\n",
    "        T.children = Dict()\n",
    "        T.grad = nothing\n",
    "        T.index_select_indices = nothing\n",
    "        \n",
    "        if !(isnothing(creators))\n",
    "            for c in creators\n",
    "                if haskey(c.children, T.id)\n",
    "                    c.children[T.id] += 1\n",
    "                else\n",
    "                    c.children[T.id] = 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        return T\n",
    "    end\n",
    "end\n",
    "\n",
    "function all_children_grads_accounted_for(t::Tensor)\n",
    "    for (id, cnt) in t.children\n",
    "        if (cnt != 0)\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function backward(t::Tensor, grad=nothing, grad_origin=nothing)\n",
    "    if t.autograd\n",
    "        if isnothing(grad)\n",
    "            grad = Tensor(ones(size(t.data)))\n",
    "        end\n",
    "    \n",
    "        if !(isnothing(grad_origin))\n",
    "            if t.children[grad_origin.id] == 0\n",
    "                return\n",
    "                throw(\"cannot backprop more than once\")\n",
    "            else\n",
    "                t.children[grad_origin.id] -= 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if isnothing(t.grad)\n",
    "            t.grad = grad\n",
    "        else\n",
    "            t.grad += grad\n",
    "        end\n",
    "        \n",
    "        # grads must not have grads of their own\n",
    "        @assert !grad.autograd\n",
    "        \n",
    "        # only continue backpropping if there's something to\n",
    "        # backprop into and if all gradients (from children)\n",
    "        # are accounted for override waiting for children if\n",
    "        # \"backprop\" was called on this variable directly\n",
    "        \n",
    "        if (!isnothing(t.creators) && (all_children_grads_accounted_for(t) || isnothing(grad_origin)))\n",
    "            if t.creation_op == \"add\"\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], t.grad, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"sub\"\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], -t.grad, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"mul\"\n",
    "                new_ = t.grad .* t.creators[2]\n",
    "                backward(t.creators[1], new_, t)\n",
    "                new_ = t.grad .* t.creators[1]\n",
    "                backward(t.creators[2], new_, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"mm\"\n",
    "                c1 = t.creators[1]\n",
    "                c2 = t.creators[2]\n",
    "                new_ =  t.grad * c2' ################\n",
    "                backward(c1, new_)\n",
    "                new_ = c1' * t.grad\n",
    "                backward(c2, new_)\n",
    "            end\n",
    "                  \n",
    "            if t.creation_op == \"transpose\"\n",
    "                backward(t.creators[1], t.grad')\n",
    "            end\n",
    "            \n",
    "            if occursin(\"sum\", t.creation_op)\n",
    "                dim = parse(Int, split(t.creation_op, \"_\")[2])\n",
    "                backward(t.creators[1], expand(t.grad, dim, size(t.creators[1].data)[dim]))\n",
    "            end\n",
    "            \n",
    "            if occursin(\"expand\", t.creation_op)\n",
    "                dim = parse(Int, split(t.creation_op, \"_\")[2])\n",
    "                ndims_cr = ndims(t.creators[1].data)\n",
    "                backward(t.creators[1], dropdims(sum(t.grad;dims=dim);dims=dim, ndims_cr=ndims_cr))\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"neg\"\n",
    "                backward(t.creators[1], -t.grad)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"sigmoid\"\n",
    "                ones_ = Tensor(ones(size(t.grad.data)))\n",
    "                backward(t.creators[1], t.grad .* t .* (ones_ - t) )\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"tanh\"\n",
    "                ones_ = Tensor(ones(size(t.grad.data)))\n",
    "                backward(t.creators[1], t.grad .* (ones_ - (t .* t)))\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"index_select\"\n",
    "                new_grad = zeros(size(t.creators[1]))\n",
    "                indices = t.index_select_indices.data\n",
    "                major_chunks = partition(1:size(t.grad,2),length(indices))\n",
    "                grad_chunks = [t.grad.data[:,inds][:,j]  for(i,inds) in enumerate(major_chunks) for j=1:size(inds)[1]]\n",
    "    \n",
    "                for (i,ind) in enumerate(flatten(indices))\n",
    "                    new_grad[:,ind] +=  grad_chunks[i]\n",
    "                end\n",
    "                backward(t.creators[1], Tensor(new_grad))\n",
    "            end\n",
    "            if t.creation_op == \"cross_entropy\"\n",
    "                dx = t.softmax_output .- t.target_dist\n",
    "                backward(t.creators[1], Tensor(dx))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "                        \n",
    "size(a::Tensor) = size(a.data)\n",
    "size(a::Tensor, ind::Int) = size(a.data, ind)\n",
    "\n",
    "function +(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data + b.data; autograd=true, creators=[a,b], creation_op = \"add\")\n",
    "    end\n",
    "    return Tensor(a.data+b.data)\n",
    "end\n",
    "\n",
    "function -(a::Tensor)\n",
    "    if (a.autograd)\n",
    "        return Tensor(a.data .* -1; autograd=true, creators=[a], creation_op = \"neg\")\n",
    "    end\n",
    "    return Tensor(a.data .* -1)\n",
    "end\n",
    "\n",
    "function -(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data - b.data; autograd=true, creators=[a,b], creation_op = \"sub\")\n",
    "    end\n",
    "    return Tensor(a.data-b.data)\n",
    "end\n",
    "\n",
    "#element-wise multiplication\n",
    "function broadcasted(f::typeof(*), a::Tensor, b::Tensor)\n",
    "    new_data = zeros(size(a.data))\n",
    "    for i=1:length(new_data)\n",
    "        new_data[i] = f(a.data[i] ,b.data[i])\n",
    "    end\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a,b], creation_op =\"mul\")\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "function broadcasted(f::typeof(-), a::Tensor, b::Tensor)\n",
    "    new_data = zeros(size(a.data))\n",
    "    for i=1:length(new_data)\n",
    "        new_data[i] = -(a.data[i] ,b.data[i])\n",
    "    end\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a,b], creation_op =\"sub\")\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "function sum(a::Tensor; dims=dims)\n",
    "    new_ = dropdims(sum(a.data ;dims=dims), dims = tuple(findall(size(a) .== 1)...))\n",
    "    if (a.autograd)\n",
    "        return Tensor(new_; autograd=true, creators=[a], creation_op = \"sum_\"*string(dims))\n",
    "    end\n",
    "    return Tensor(new_)\n",
    "end\n",
    "\n",
    "function dropdims(a::Tensor;dims=dims,ndims_cr=ndims_cr)\n",
    "    if ndims(a.data) == ndims_cr\n",
    "        return a\n",
    "    end\n",
    "    if (a.autograd)\n",
    "        return Tensor(dropdims(a.data ;dims=dims); autograd=true, creators=[a], creation_op = \"dropdims\")\n",
    "    end\n",
    "    return Tensor(dropdims(a.data ;dims=dims))\n",
    "end\n",
    "\n",
    "function expand(a::Tensor, dim, copies)\n",
    "    sz = size(a)\n",
    "    rep = ntuple(d->d==dim ? copies : 1, length(sz)+1)\n",
    "    new_size = ntuple(d->d<dim ? sz[d] : d == dim ? 1 : sz[d-1], length(sz)+1)\n",
    "    new_data =  repeat(reshape(a.data, new_size), outer=rep)\n",
    "    if (a.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a], creation_op = \"expand_\"*string(dim))\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "#transpose\n",
    "function adjoint(a::Tensor)\n",
    "    if (a.autograd)\n",
    "        return Tensor(a.data';autograd=true, creators=[a], creation_op = \"transpose\")\n",
    "    end\n",
    "    return Tensor(a.data')\n",
    "end\n",
    "\n",
    "#matrix multiply \n",
    "function *(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data * b.data; autograd=true, creators=[a,b], creation_op = \"mm\")\n",
    "    end\n",
    "    return Tensor(a.data * b.data)\n",
    "end\n",
    "\n",
    "\n",
    "function index_select_helper(a::Array, indices)\n",
    "    return reduce(hcat,map(ind -> a[:,ind], indices))\n",
    "end\n",
    "\n",
    "function index_select(a::Tensor, indices::Tensor)\n",
    "    new_ = index_select_helper(a.data, indices.data)\n",
    "    if (a.autograd)\n",
    "        T = Tensor(new_, autograd=true, creators=[a], creation_op = \"index_select\")\n",
    "        T.index_select_indices = indices\n",
    "        return T\n",
    "    end\n",
    "    return Tensor(new_)\n",
    "end\n",
    "\n",
    "println(t::Tensor) = println(t.data)\n",
    "show(io::IO,m::MIME\"text/plain\",a::Tensor) = show(io,m,a.data)\n",
    "                        \n",
    "abstract type Layer end\n",
    "\n",
    "function get_parameters(l::Layer)\n",
    "    return l.parameters\n",
    "end\n",
    "\n",
    "mutable struct Linear <: Layer\n",
    "    W\n",
    "    b\n",
    "    use_bias\n",
    "    parameters\n",
    "                            \n",
    "    function Linear(n_inputs, n_outputs;bias=true)\n",
    "        linear = new()\n",
    "        linear.use_bias = bias\n",
    "        linear.W = Tensor(randn(n_outputs, n_inputs) .* sqrt(2.0/n_inputs), autograd=true)\n",
    "        if bias\n",
    "            linear.b = Tensor(zeros(n_outputs), autograd=true) \n",
    "            linear.parameters = [linear.W,linear.b]\n",
    "        else\n",
    "            linear.parameters = [linear.W]\n",
    "        end\n",
    "        return linear\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(l::Linear, input)\n",
    "    if l.use_bias\n",
    "        return (l.W * input)  + expand(l.b,2,size(input.data, 2))\n",
    "    end\n",
    "    return l.W * input\n",
    "end                        \n",
    "\n",
    "                        \n",
    "mutable struct Sequential <: Layer\n",
    "    layers\n",
    "    function Sequential(layers)\n",
    "        return new(layers)\n",
    "    end\n",
    "end\n",
    "\n",
    "function add(s::Sequential, layer)\n",
    "    push!(s.layers, layer)\n",
    "end\n",
    "\n",
    "function forward(s::Sequential, input)\n",
    "    for layer in s.layers\n",
    "        input = forward(layer, input)\n",
    "    end\n",
    "    return input\n",
    "end\n",
    "\n",
    "function get_parameters(s::Sequential)\n",
    "    parameters = [get_parameters(layer) for layer in s.layers]\n",
    "    return collect(Iterators.flatten(parameters))\n",
    "end\n",
    "\n",
    "mutable struct SGD\n",
    "    parameters\n",
    "    alpha\n",
    "    SGD(parameters, alpha) = new(parameters, alpha)\n",
    "end\n",
    "\n",
    "function zero!(opt::SGD)\n",
    "    for p in opt.parameters\n",
    "        p.grad.data .*= 0.0\n",
    "    end\n",
    "end\n",
    "\n",
    "function step(opt::SGD, zero=true)\n",
    "    for p in opt.parameters\n",
    "        p.data -= (p.grad.data .* opt.alpha)\n",
    "        if zero\n",
    "            p.grad.data .*= 0.0\n",
    "        end\n",
    "    end\n",
    "end\n",
    "                        \n",
    "σ(x) = 1/(1+exp(-x))                        \n",
    "\n",
    "struct Tanh <: Layer\n",
    "    Tanh() = new()\n",
    "end\n",
    "\n",
    "struct Sigmoid <: Layer\n",
    "    Sigmoid() = new()\n",
    "end\n",
    "\n",
    "function get_parameters(l::Tanh)\n",
    "    return []\n",
    "end\n",
    "\n",
    "function get_parameters(l::Sigmoid)\n",
    "    return []\n",
    "end\n",
    "\n",
    "function forward(l::Sigmoid, a::Tensor)\n",
    "    if a.autograd\n",
    "        return Tensor(σ.(a.data); autograd=true, creators=[a], creation_op = \"sigmoid\")\n",
    "    end\n",
    "    return Tensor(σ.(a.data))\n",
    "end\n",
    "        \n",
    "function forward(l::Tanh, a::Tensor)\n",
    "    if a.autograd\n",
    "        return Tensor(tanh.(a.data); autograd=true, creators=[a], creation_op = \"tanh\")\n",
    "    end\n",
    "    return Tensor(tanh.(a.data))\n",
    "end    \n",
    "                        \n",
    "                        \n",
    "mutable struct Embedding <: Layer\n",
    "    vocab_size\n",
    "    dim\n",
    "    weight\n",
    "    parameters\n",
    "    # this random initialiation style is just a convention from word2vec\n",
    "    function Embedding(dim, vocab_size) \n",
    "        E = new(vocab_size, dim, Tensor((randn(dim, vocab_size) .- 0.5) ./ dim; autograd=true))\n",
    "        E.parameters = [E.weight]\n",
    "        return E\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(E::Embedding, indices)\n",
    "    return index_select(E.weight, indices)\n",
    "end\n",
    "                        \n",
    "using Statistics: mean\n",
    "using LinearAlgebra: I\n",
    "function softmax(x)\n",
    "    temp = exp.(x)\n",
    "    return temp ./ sum(temp;dims=1)\n",
    "end\n",
    "\n",
    "struct CrossEntropyLoss \n",
    "    CrossEntropyLoss() = new()\n",
    "end\n",
    "\n",
    "function forward(l::CrossEntropyLoss, a::Tensor, target::Tensor)\n",
    "    softmax_output = softmax(a.data)\n",
    "    log_out = log.(softmax_output)\n",
    "    sz = size(a.data, 1)\n",
    "    identity = 1.0 .* Matrix(I, (sz, sz))\n",
    "    target_dist = reshape(identity[:,target.data],(size(a.data)))\n",
    "    loss = -mean(sum(log_out .* target_dist;dims=1))\n",
    "    if a.autograd\n",
    "        loss = Tensor(loss; autograd=true, creators=[a], creation_op = \"cross_entropy\")\n",
    "        loss.softmax_output = softmax_output\n",
    "        loss.target_dist = target_dist\n",
    "        return loss\n",
    "    end\n",
    "    return Tensor(loss)\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct RNNCell <: Layer\n",
    "    n_hidden\n",
    "    \n",
    "    activation\n",
    "    \n",
    "    w_ih\n",
    "    w_hh\n",
    "    w_ho\n",
    "    \n",
    "    parameters\n",
    "    \n",
    "    function RNNCell(n_inputs, n_hidden, n_output, activation=\"sigmoid\")\n",
    "        if activation == \"sigmoid\"\n",
    "            act = Sigmoid()\n",
    "        elseif activation == \"tanh\"\n",
    "            act = Tanh()\n",
    "        else\n",
    "            throw(\"Non-linearity not found\")\n",
    "        end\n",
    "        \n",
    "        parameters = []\n",
    "\n",
    "        w_ih = Linear(n_inputs, n_hidden)\n",
    "        w_hh = Linear(n_hidden, n_hidden)\n",
    "        w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        push!(parameters, get_parameters(w_ih))\n",
    "        push!(parameters, get_parameters(w_hh))\n",
    "        push!(parameters, get_parameters(w_ho))\n",
    "        parameters = collect(Iterators.flatten(parameters))\n",
    "        return new(n_hidden, act, w_ih, w_hh, w_ho, parameters)\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(rnn::RNNCell, input::Tensor, hidden::Tensor)\n",
    "    from_prev_hidden = forward(rnn.w_hh, hidden)\n",
    "    combined = forward(rnn.w_ih, input) + from_prev_hidden\n",
    "    new_hidden = forward(rnn.activation, combined)\n",
    "    output = forward(rnn.w_ho, new_hidden)\n",
    "    return output, new_hidden\n",
    "end\n",
    "\n",
    "function init_hidden(rnn::RNNCell; batch_size=1)\n",
    "    return Tensor(zeros(rnn.n_hidden, batch_size), autograd=true)\n",
    "end\n",
    "\n",
    "mutable struct LSTMCell <: Layer\n",
    "    \n",
    "    n_hidden\n",
    "    \n",
    "    xf\n",
    "    xi\n",
    "    xo\n",
    "    xc\n",
    "    \n",
    "    hf\n",
    "    hi\n",
    "    ho\n",
    "    hc\n",
    "    \n",
    "    w_ho\n",
    "    parameters\n",
    "    sigmoid\n",
    "    tanh\n",
    "    \n",
    "    function LSTMCell(n_inputs, n_hidden, n_output)\n",
    "\n",
    "        xf = Linear(n_inputs, n_hidden)\n",
    "        xi = Linear(n_inputs, n_hidden)\n",
    "        xo = Linear(n_inputs, n_hidden)        \n",
    "        xc = Linear(n_inputs, n_hidden) \n",
    "        \n",
    "        hf = Linear(n_hidden, n_hidden; bias=false)\n",
    "        hi = Linear(n_hidden, n_hidden; bias=false)\n",
    "        ho = Linear(n_hidden, n_hidden; bias=false)\n",
    "        hc = Linear(n_hidden, n_hidden; bias=false) \n",
    "        \n",
    "        w_ho = Linear(n_hidden, n_output; bias=false)\n",
    "        \n",
    "        parameters = [get_parameters(i) for i in [xf, xi, xo, xc, hf, hi, hc, w_ho]]\n",
    "        parameters = collect(Iterators.flatten(parameters))\n",
    "        \n",
    "        return new(n_hidden, xf, xi, xo, xc, hf, hi, ho, hc, w_ho, parameters, Sigmoid(), Tanh())\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(lstm::LSTMCell, input::Tensor, hidden)\n",
    "    \n",
    "    prev_hidden = hidden[1]        \n",
    "    prev_cell = hidden[2]\n",
    "    \n",
    "    f = forward(lstm.xf, input) + forward(lstm.sigmoid, forward(lstm.hf, prev_hidden))\n",
    "    i = forward(lstm.xi, input) + forward(lstm.sigmoid, forward(lstm.hi, prev_hidden))\n",
    "    o = forward(lstm.xo, input) + forward(lstm.sigmoid, forward(lstm.ho, prev_hidden))\n",
    "    g = forward(lstm.xc, input) + forward(lstm.tanh, forward(lstm.hc, prev_hidden))\n",
    "    \n",
    "    c = (f .* prev_cell) + (i .* g)\n",
    "\n",
    "    h = o .* forward(lstm.tanh, c)\n",
    "    \n",
    "    output = forward(lstm.w_ho, h)\n",
    "    \n",
    "    return output, (h, c)\n",
    "end\n",
    "\n",
    "function init_hidden(lstm; batch_size=1)\n",
    "    init_hidden = Tensor(zeros(lstm.n_hidden, batch_size), autograd=true)\n",
    "    init_cell   = Tensor(zeros(lstm.n_hidden, batch_size), autograd=true)\n",
    "    \n",
    "    init_hidden.data[1,:] .+= 1\n",
    "    init_cell.data[1,:] .+= 1\n",
    "    return (init_hidden, init_cell)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RNN Character Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = read(\"shakespear.txt\", String)\n",
    "vocab = collect(Set(raw))\n",
    "word2index = Dict()\n",
    "for (i,word) in enumerate(vocab)\n",
    "    word2index[word]=i\n",
    "end\n",
    "\n",
    "using Random:seed!;seed!(0)\n",
    "embed = Embedding(512, length(vocab))\n",
    "model = LSTMCell(512, 512, length(vocab))\n",
    "model.w_ho.W.data .*= 0\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(cat(get_parameters(model), get_parameters(embed); dims=1), 0.05);\n",
    "\n",
    "function generate_sample(;n=30,init_char=' ')\n",
    "    s = \"\"\n",
    "    hidden = init_hidden(model, batch_size=1)\n",
    "    input = Tensor([word2index[init_char]])\n",
    "    for i=1:n\n",
    "        rnn_input = forward(embed, input)\n",
    "        output, hidden = forward(model, rnn_input, hidden)\n",
    "        output.data .*=5\n",
    "        temp_dist = softmax(output.data)\n",
    "        temp_dist ./= sum(temp_dist)\n",
    "        \n",
    "        m = argmax(temp_dist .>  rand()).I[1]\n",
    "        \n",
    "#         m = argmax(output.data).I[1]\n",
    "        c = vocab[m]\n",
    "        input = Tensor([m])\n",
    "        s *= c\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "indices = map(x->word2index[x], collect(raw))\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = trunc(Int,size(indices,1)/batch_size)\n",
    "trimmed_indices = indices[1:n_batches*batch_size]\n",
    "\n",
    "input_batched_indices = reshape(indices[1:n_batches*batch_size], (batch_size, n_batches))\n",
    "target_batched_indices = reshape(indices[2:n_batches*batch_size+1], (batch_size, n_batches))\n",
    "\n",
    "n_bptt = trunc(Int,((n_batches-1) / bptt))\n",
    "\n",
    "input_batches = permutedims(reshape(input_batched_indices[:,1:n_bptt*bptt], (n_bptt, bptt, batch_size)), (3,1,2))\n",
    "input_batches = reshape(input_batches, (batch_size,bptt,n_bptt))\n",
    "\n",
    "target_batches = permutedims(reshape(target_batched_indices[:,1:n_bptt*bptt], (n_bptt, bptt, batch_size)), (3,1,2))\n",
    "target_batches = reshape(target_batches, (batch_size,bptt,n_bptt))\n",
    "min_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(iterations=400;batch_size=16,bptt=25,min_loss=1000)\n",
    "    for iter=1:iterations\n",
    "        \n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = init_hidden(model, batch_size=batch_size)\n",
    "        \n",
    "        for batch_i=1:size(input_batches,3)\n",
    "            losses = []\n",
    "            hidden = [Tensor(hidden[1].data, autograd=true), Tensor(hidden[2].data, autograd=true)]\n",
    "\n",
    "            for t=1:bptt\n",
    "                input = Tensor(input_batches[:,t,batch_i], autograd=true)\n",
    "                rnn_input = forward(embed, input)\n",
    "                output, hidden = forward(model, rnn_input, hidden)\n",
    "                \n",
    "                target = Tensor(target_batches[:,t,batch_i], autograd=true)\n",
    "                batch_loss = forward(criterion, output, target)\n",
    "                \n",
    "                if t==1\n",
    "                    push!(losses, batch_loss)\n",
    "                else\n",
    "                    push!(losses, batch_loss + losses[end])\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            loss = losses[end]\n",
    "            backward(loss)\n",
    "            step(optim)\n",
    "            total_loss += loss.data/bptt\n",
    "            \n",
    "            epoch_loss = exp(total_loss / batch_i)\n",
    "            \n",
    "            if(epoch_loss < min_loss)\n",
    "                min_loss = epoch_loss\n",
    "            end\n",
    "            \n",
    "            log = \"Iter: $(iter)\"\n",
    "            log *= \" - Alpha: $(round(optim.alpha;digits=5))\"\n",
    "            log *= \" - Batch $(batch_i)/$(size(input_batches,3))\"\n",
    "            log *= \" - Min Loss: $(string(min_loss)[1:5])\"\n",
    "            log *= \" - Loss: $(epoch_loss)\"\n",
    "            if batch_i == 1\n",
    "                log*= \" - \" * replace(generate_sample(n=70, init_char='T'),'\\n' => \" \")\n",
    "            end\n",
    "            if ((batch_i-1) % 10 == 0) || batch_i == size(input_batches,3)\n",
    "                print(log,\"\\r\")\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        println()\n",
    "        optim.alpha *= 0.99\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 - Alpha: 0.05 - Batch 249/249 - Min Loss: 18.74 - Loss: 18.742586693715285 llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "Iter: 2 - Alpha: 0.0495 - Batch 249/249 - Min Loss: 12.22 - Loss: 12.813928907550277 le the the the the the the the the the the the the the the the the the\n",
      "Iter: 3 - Alpha: 0.049 - Batch 249/249 - Min Loss: 11.48 - Loss: 12.338460119964218all the the the the the the the the the the the the the the the the th\n",
      "Iter: 4 - Alpha: 0.04851 - Batch 249/249 - Min Loss: 11.48 - Loss: 12.411838394413333 heleleleleleleleleleleleeleleleed the the the therelele the the the th\n",
      "Iter: 5 - Alpha: 0.04803 - Batch 249/249 - Min Loss: 11.48 - Loss: 12.482593177994267 he tele tle tere tere tle tere tere tle tere tere tle tle tere tere tl\n",
      "Iter: 6 - Alpha: 0.04755 - Batch 249/249 - Min Loss: 11.13 - Loss: 12.364624121192073 he the the the the the the the the the the the the the the the the the\n",
      "Iter: 7 - Alpha: 0.04707 - Batch 249/249 - Min Loss: 11.05 - Loss: 12.340146332209741 he the le the the the the the the the the the the the the the the the \n",
      "Iter: 8 - Alpha: 0.0466 - Batch 249/249 - Min Loss: 11.05 - Loss: 13.081008808752069 hatleleletes the the the the letleleleleleletederterthe the the the th\n",
      "Iter: 9 - Alpha: 0.04614 - Batch 249/249 - Min Loss: 11.05 - Loss: 12.841340676013967 hele, thele, thele, thele, theee, theeeell theeeell thelell theeell th\n",
      "Iter: 10 - Alpha: 0.04568 - Batch 249/249 - Min Loss: 11.05 - Loss: 12.995340443317776 he the the le the the the the le the the the le the the le the the the\n"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 - Alpha: 0.03 - Batch 249/249 - Min Loss: 11.61 - Loss: 11.653433124797726 hos de le le le are s leleale le s leay s you le ale s you le e ale le\n",
      "Iter: 2 - Alpha: 0.0297 - Batch 249/249 - Min Loss: 10.43 - Loss: 11.577565720204944 he the le the the le the the the le the le the the the le the the the \n",
      "Iter: 3 - Alpha: 0.0294 - Batch 249/249 - Min Loss: 10.43 - Loss: 11.884919209164313 he the the the the the the the the the the the the le the the the the \n",
      "Iter: 4 - Alpha: 0.02911 - Batch 249/249 - Min Loss: 10.43 - Loss: 12.449659014337971 IO:  Have the le the the the the the the the the the the the thentertl\n",
      "Iter: 5 - Alpha: 0.02882 - Batch 249/249 - Min Loss: 10.43 - Loss: 12.384929731529825 hele the the the le le the le the le the the the le le the le the le l\n",
      "Iter: 6 - Alpha: 0.02853 - Batch 249/249 - Min Loss: 10.43 - Loss: 12.857174618178181 here the entle the the lesesele the eent le the ent ent ele the le the\n",
      "Iter: 7 - Alpha: 0.02824 - Batch 249/249 - Min Loss: 10.43 - Loss: 12.354982098341678 hlele theelemlele theeme, let thee, let letle theele letle theelelelem\n",
      "Iter: 8 - Alpha: 0.02796 - Batch 249/249 - Min Loss: 10.43 - Loss: 12.670611814951819 he the the the the the the the the the the the the the let le the the \n",
      "Iter: 9 - Alpha: 0.02768 - Batch 249/249 - Min Loss: 10.43 - Loss: 12.413195551091881he le the the the the the the the the theeeeeele thele the the the the\n",
      "Iter: 10 - Alpha: 0.02741 - Batch 249/249 - Min Loss: 10.43 - Loss: 12.194318472524255he the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_sample(;n=30,init_char=' ')\n",
    "    s = \"\"\n",
    "    hidden = init_hidden(model, batch_size=1)\n",
    "    input = Tensor([word2index[init_char]])\n",
    "    for i=1:n\n",
    "        rnn_input = forward(embed, input)\n",
    "        output, hidden = forward(model, rnn_input, hidden)\n",
    "#         output.data .*=20\n",
    "        temp_dist = softmax(output.data)\n",
    "        temp_dist ./= sum(temp_dist)\n",
    "        if rand() > 0.5\n",
    "            m = argmax(temp_dist).I[1]\n",
    "        else\n",
    "            m = argmax(temp_dist .>  rand()).I[1]\n",
    "        end\n",
    "        c = vocab[m]\n",
    "#         println(m)\n",
    "        input = Tensor([m])\n",
    "        s *= c\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
