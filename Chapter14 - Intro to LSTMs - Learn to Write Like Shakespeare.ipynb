{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 6 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "import Base:+,-,*,println, sum, broadcasted, size, adjoint, show, dropdims, tanh\n",
    "using Base.Iterators:partition, flatten\n",
    "\n",
    "mutable struct Tensor\n",
    "    data\n",
    "    autograd\n",
    "    creators\n",
    "    creation_op\n",
    "    id\n",
    "    children\n",
    "    grad \n",
    "    index_select_indices\n",
    "    softmax_output\n",
    "    target_dist\n",
    "    \n",
    "    function Tensor(data; autograd=false, creators=nothing, creation_op = nothing, id=nothing)\n",
    "        if isnothing(id)\n",
    "            id = rand(1:100000)\n",
    "        end\n",
    "        T = new(data, autograd, creators, creation_op, id)\n",
    "        T.children = Dict()\n",
    "        T.grad = nothing\n",
    "        T.index_select_indices = nothing\n",
    "        \n",
    "        if !(isnothing(creators))\n",
    "            for c in creators\n",
    "                if haskey(c.children, T.id)\n",
    "                    c.children[T.id] += 1\n",
    "                else\n",
    "                    c.children[T.id] = 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        return T\n",
    "    end\n",
    "end\n",
    "\n",
    "function all_children_grads_accounted_for(t::Tensor)\n",
    "    for (id, cnt) in t.children\n",
    "        if (cnt != 0)\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function backward(t::Tensor, grad=nothing, grad_origin=nothing)\n",
    "    if t.autograd\n",
    "        if isnothing(grad)\n",
    "            grad = Tensor(ones(size(t.data)))\n",
    "        end\n",
    "    \n",
    "        if !(isnothing(grad_origin))\n",
    "            if t.children[grad_origin.id] == 0\n",
    "                throw(\"cannot backprop more than once\")\n",
    "            else\n",
    "                t.children[grad_origin.id] -= 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if isnothing(t.grad)\n",
    "            t.grad = grad\n",
    "        else\n",
    "            t.grad += grad\n",
    "        end\n",
    "        \n",
    "        # grads must not have grads of their own\n",
    "        @assert !grad.autograd\n",
    "        \n",
    "        # only continue backpropping if there's something to\n",
    "        # backprop into and if all gradients (from children)\n",
    "        # are accounted for override waiting for children if\n",
    "        # \"backprop\" was called on this variable directly\n",
    "        \n",
    "        if (!isnothing(t.creators) && (all_children_grads_accounted_for(t) || isnothing(grad_origin)))\n",
    "            if t.creation_op == \"add\"\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], t.grad, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"sub\"\n",
    "                backward(t.creators[1], t.grad, t)\n",
    "                backward(t.creators[2], -t.grad, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"mul\"\n",
    "                new_ = t.grad .* t.creators[2]\n",
    "                backward(t.creators[1], new_, t)\n",
    "                new_ = t.grad .* t.creators[1]\n",
    "                backward(t.creators[2], new_, t)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"mm\"\n",
    "                c1 = t.creators[1]\n",
    "                c2 = t.creators[2]\n",
    "                new_ =  t.grad * c2' ################\n",
    "                backward(c1, new_)\n",
    "                new_ = c1' * t.grad\n",
    "                backward(c2, new_)\n",
    "            end\n",
    "                  \n",
    "            if t.creation_op == \"transpose\"\n",
    "                backward(t.creators[1], t.grad')\n",
    "            end\n",
    "            \n",
    "            if occursin(\"sum\", t.creation_op)\n",
    "                dim = parse(Int, split(t.creation_op, \"_\")[2])\n",
    "                backward(t.creators[1], expand(t.grad, dim, size(t.creators[1].data)[dim]))\n",
    "            end\n",
    "            \n",
    "            if occursin(\"expand\", t.creation_op)\n",
    "                dim = parse(Int, split(t.creation_op, \"_\")[2])\n",
    "                ndims_cr = ndims(t.creators[1].data)\n",
    "                backward(t.creators[1], dropdims(sum(t.grad;dims=dim);dims=dim, ndims_cr=ndims_cr))\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"neg\"\n",
    "                backward(t.creators[1], -t.grad)\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"sigmoid\"\n",
    "                ones_ = Tensor(ones(size(t.grad.data)))\n",
    "                backward(t.creators[1], t.grad .* t .* (ones_ - t) )\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"tanh\"\n",
    "                ones_ = Tensor(ones(size(t.grad.data)))\n",
    "                backward(t.creators[1], t.grad .* (ones_ - (t .* t)))\n",
    "            end\n",
    "            \n",
    "            if t.creation_op == \"index_select\"\n",
    "                new_grad = zeros(size(t.creators[1]))\n",
    "                indices = t.index_select_indices.data\n",
    "                major_chunks = partition(1:size(t.grad,2),length(indices))\n",
    "                grad_chunks = [t.grad.data[:,inds][:,j]  for(i,inds) in enumerate(major_chunks) for j=1:size(inds)[1]]\n",
    "    \n",
    "                for (i,ind) in enumerate(flatten(indices))\n",
    "                    new_grad[:,ind] +=  grad_chunks[i]\n",
    "                end\n",
    "                backward(t.creators[1], Tensor(new_grad))\n",
    "            end\n",
    "            if t.creation_op == \"cross_entropy\"\n",
    "                dx = t.softmax_output .- t.target_dist\n",
    "                backward(t.creators[1], Tensor(dx))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "                        \n",
    "size(a::Tensor) = size(a.data)\n",
    "size(a::Tensor, ind::Int) = size(a.data, ind)\n",
    "\n",
    "function +(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data + b.data; autograd=true, creators=[a,b], creation_op = \"add\")\n",
    "    end\n",
    "    return Tensor(a.data+b.data)\n",
    "end\n",
    "\n",
    "function -(a::Tensor)\n",
    "    if (a.autograd)\n",
    "        return Tensor(a.data .* -1; autograd=true, creators=[a], creation_op = \"neg\")\n",
    "    end\n",
    "    return Tensor(a.data .* -1)\n",
    "end\n",
    "\n",
    "function -(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data - b.data; autograd=true, creators=[a,b], creation_op = \"sub\")\n",
    "    end\n",
    "    return Tensor(a.data-b.data)\n",
    "end\n",
    "\n",
    "#element-wise multiplication\n",
    "function broadcasted(f::typeof(*), a::Tensor, b::Tensor)\n",
    "    new_data = zeros(size(a.data))\n",
    "    for i=1:length(new_data)\n",
    "        new_data[i] = f(a.data[i] ,b.data[i])\n",
    "    end\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a,b], creation_op =\"mul\")\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "function broadcasted(f::typeof(-), a::Tensor, b::Tensor)\n",
    "    new_data = zeros(size(a.data))\n",
    "    for i=1:length(new_data)\n",
    "        new_data[i] = -(a.data[i] ,b.data[i])\n",
    "    end\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a,b], creation_op =\"sub\")\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "function sum(a::Tensor; dims=dims)\n",
    "    new_ = dropdims(sum(a.data ;dims=dims), dims = tuple(findall(size(a) .== 1)...))\n",
    "    if (a.autograd)\n",
    "        return Tensor(new_; autograd=true, creators=[a], creation_op = \"sum_\"*string(dims))\n",
    "    end\n",
    "    return Tensor(new_)\n",
    "end\n",
    "\n",
    "function dropdims(a::Tensor;dims=dims,ndims_cr=ndims_cr)\n",
    "    if ndims(a.data) == ndims_cr\n",
    "        return a\n",
    "    end\n",
    "    if (a.autograd)\n",
    "        return Tensor(dropdims(a.data ;dims=dims); autograd=true, creators=[a], creation_op = \"dropdims\")\n",
    "    end\n",
    "    return Tensor(dropdims(a.data ;dims=dims))\n",
    "end\n",
    "\n",
    "function expand(a::Tensor, dim, copies)\n",
    "    sz = size(a)\n",
    "    rep = ntuple(d->d==dim ? copies : 1, length(sz)+1)\n",
    "    new_size = ntuple(d->d<dim ? sz[d] : d == dim ? 1 : sz[d-1], length(sz)+1)\n",
    "    new_data =  repeat(reshape(a.data, new_size), outer=rep)\n",
    "    if (a.autograd)\n",
    "        return Tensor(new_data; autograd=true, creators=[a], creation_op = \"expand_\"*string(dim))\n",
    "    end\n",
    "    return Tensor(new_data)\n",
    "end\n",
    "\n",
    "#transpose\n",
    "function adjoint(a::Tensor)\n",
    "    if (a.autograd)\n",
    "        return Tensor(a.data';autograd=true, creators=[a], creation_op = \"transpose\")\n",
    "    end\n",
    "    return Tensor(a.data')\n",
    "end\n",
    "\n",
    "#matrix multiply \n",
    "function *(a::Tensor, b::Tensor)\n",
    "    if (a.autograd && b.autograd)\n",
    "        return Tensor(a.data * b.data; autograd=true, creators=[a,b], creation_op = \"mm\")\n",
    "    end\n",
    "    return Tensor(a.data * b.data)\n",
    "end\n",
    "\n",
    "\n",
    "function index_select_helper(a::Array, indices)\n",
    "    return reduce(hcat,map(ind -> a[:,ind], indices))\n",
    "end\n",
    "\n",
    "function index_select(a::Tensor, indices::Tensor)\n",
    "    new_ = index_select_helper(a.data, indices.data)\n",
    "    if (a.autograd)\n",
    "        T = Tensor(new_, autograd=true, creators=[a], creation_op = \"index_select\")\n",
    "        T.index_select_indices = indices\n",
    "        return T\n",
    "    end\n",
    "    return Tensor(new_)\n",
    "end\n",
    "\n",
    "println(t::Tensor) = println(t.data)\n",
    "show(io::IO,m::MIME\"text/plain\",a::Tensor) = show(io,m,a.data)\n",
    "                        \n",
    "abstract type Layer end\n",
    "\n",
    "function get_parameters(l::Layer)\n",
    "    return l.parameters\n",
    "end\n",
    "\n",
    "mutable struct Linear <: Layer\n",
    "    W\n",
    "    b\n",
    "    use_bias\n",
    "    parameters\n",
    "                            \n",
    "    function Linear(n_inputs, n_outputs;bias=true)\n",
    "        linear = new()\n",
    "        linear.use_bias = bias\n",
    "        linear.W = Tensor(randn(n_outputs, n_inputs) .* sqrt(2.0/n_inputs), autograd=true)\n",
    "        if bias\n",
    "            linear.b = Tensor(zeros(n_outputs), autograd=true) \n",
    "            linear.parameters = [linear.W,linear.b]\n",
    "        else\n",
    "            linear.parameters = [linear.W]\n",
    "        end\n",
    "        return linear\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(l::Linear, input)\n",
    "    if l.use_bias\n",
    "        return (l.W * input)  + expand(l.b,2,size(input.data, 2))\n",
    "    end\n",
    "    return l.W * input\n",
    "end                        \n",
    "\n",
    "                        \n",
    "mutable struct Sequential <: Layer\n",
    "    layers\n",
    "    function Sequential(layers)\n",
    "        return new(layers)\n",
    "    end\n",
    "end\n",
    "\n",
    "function add(s::Sequential, layer)\n",
    "    push!(s.layers, layer)\n",
    "end\n",
    "\n",
    "function forward(s::Sequential, input)\n",
    "    for layer in s.layers\n",
    "        input = forward(layer, input)\n",
    "    end\n",
    "    return input\n",
    "end\n",
    "\n",
    "function get_parameters(s::Sequential)\n",
    "    parameters = [get_parameters(layer) for layer in s.layers]\n",
    "    return collect(Iterators.flatten(parameters))\n",
    "end\n",
    "\n",
    "mutable struct SGD\n",
    "    parameters\n",
    "    alpha\n",
    "    SGD(parameters, alpha) = new(parameters, alpha)\n",
    "end\n",
    "\n",
    "function zero!(opt::SGD)\n",
    "    for p in opt.parameters\n",
    "        p.grad.data .*= 0.0\n",
    "    end\n",
    "end\n",
    "\n",
    "function step(opt::SGD, zero=true)\n",
    "    for p in opt.parameters\n",
    "        p.data -= (p.grad.data .* opt.alpha)\n",
    "        if zero\n",
    "            p.grad.data .*= 0.0\n",
    "        end\n",
    "    end\n",
    "end\n",
    "                        \n",
    "σ(x) = 1/(1+exp(-x))                        \n",
    "\n",
    "struct Tanh <: Layer\n",
    "    Tanh() = new()\n",
    "end\n",
    "\n",
    "struct Sigmoid <: Layer\n",
    "    Sigmoid() = new()\n",
    "end\n",
    "\n",
    "function get_parameters(l::Tanh)\n",
    "    return []\n",
    "end\n",
    "\n",
    "function get_parameters(l::Sigmoid)\n",
    "    return []\n",
    "end\n",
    "\n",
    "function forward(l::Sigmoid, a::Tensor)\n",
    "    if a.autograd\n",
    "        return Tensor(σ.(a.data); autograd=true, creators=[a], creation_op = \"sigmoid\")\n",
    "    end\n",
    "    return Tensor(σ.(a.data))\n",
    "end\n",
    "        \n",
    "function forward(l::Tanh, a::Tensor)\n",
    "    if a.autograd\n",
    "        return Tensor(tanh.(a.data); autograd=true, creators=[a], creation_op = \"tanh\")\n",
    "    end\n",
    "    return Tensor(tanh.(a.data))\n",
    "end    \n",
    "                        \n",
    "                        \n",
    "mutable struct Embedding <: Layer\n",
    "    vocab_size\n",
    "    dim\n",
    "    weight\n",
    "    parameters\n",
    "    # this random initialiation style is just a convention from word2vec\n",
    "    function Embedding(dim, vocab_size) \n",
    "        E = new(vocab_size, dim, Tensor((randn(dim, vocab_size) .- 0.5) ./ dim; autograd=true))\n",
    "        E.parameters = [E.weight]\n",
    "        return E\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(E::Embedding, indices)\n",
    "    return index_select(E.weight, indices)\n",
    "end\n",
    "                        \n",
    "using Statistics: mean\n",
    "using LinearAlgebra: I\n",
    "function softmax(x)\n",
    "    temp = exp.(x)\n",
    "    return temp ./ sum(temp;dims=1)\n",
    "end\n",
    "\n",
    "struct CrossEntropyLoss \n",
    "    CrossEntropyLoss() = new()\n",
    "end\n",
    "\n",
    "function forward(l::CrossEntropyLoss, a::Tensor, target::Tensor)\n",
    "    softmax_output = softmax(a.data)\n",
    "    log_out = log.(softmax_output)\n",
    "    sz = size(a.data, 1)\n",
    "    identity = 1.0 .* Matrix(I, (sz, sz))\n",
    "    target_dist = reshape(identity[:,target.data],(size(a.data)))\n",
    "    loss = -mean(sum(log_out .* target_dist;dims=1))\n",
    "    if a.autograd\n",
    "        loss = Tensor(loss; autograd=true, creators=[a], creation_op = \"cross_entropy\")\n",
    "        loss.softmax_output = softmax_output\n",
    "        loss.target_dist = target_dist\n",
    "        return loss\n",
    "    end\n",
    "    return Tensor(loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_hidden (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct RNNCell <: Layer\n",
    "    n_hidden\n",
    "    \n",
    "    activation\n",
    "    \n",
    "    w_ih\n",
    "    w_hh\n",
    "    w_ho\n",
    "    \n",
    "    parameters\n",
    "    \n",
    "    function RNNCell(n_inputs, n_hidden, n_output, activation=\"sigmoid\")\n",
    "        if activation == \"sigmoid\"\n",
    "            act = Sigmoid()\n",
    "        elseif activation == \"tanh\"\n",
    "            act = Tanh()\n",
    "        else\n",
    "            throw(\"Non-linearity not found\")\n",
    "        end\n",
    "        \n",
    "        parameters = []\n",
    "\n",
    "        w_ih = Linear(n_inputs, n_hidden)\n",
    "        w_hh = Linear(n_hidden, n_hidden)\n",
    "        w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        push!(parameters, get_parameters(w_ih))\n",
    "        push!(parameters, get_parameters(w_hh))\n",
    "        push!(parameters, get_parameters(w_ho))\n",
    "        parameters = collect(Iterators.flatten(parameters))\n",
    "        return new(n_hidden, act, w_ih, w_hh, w_ho, parameters)\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(rnn::RNNCell, input::Tensor, hidden::Tensor)\n",
    "    from_prev_hidden = forward(rnn.w_hh, hidden)\n",
    "    combined = forward(rnn.w_ih, input) + from_prev_hidden\n",
    "    new_hidden = forward(rnn.activation, combined)\n",
    "    output = forward(rnn.w_ho, new_hidden)\n",
    "    return output, new_hidden\n",
    "end\n",
    "\n",
    "function init_hidden(rnn::RNNCell; batch_size=1)\n",
    "    return Tensor(zeros(rnn.n_hidden, batch_size), autograd=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RNN Character Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = read(\"shakespear.txt\", String)\n",
    "vocab = collect(Set(raw))\n",
    "word2index = Dict()\n",
    "for (i,word) in enumerate(vocab)\n",
    "    word2index[word]=i\n",
    "end\n",
    "\n",
    "using Random:seed!;seed!(0)\n",
    "embed = Embedding(512, length(vocab))\n",
    "model = RNNCell(512, 512, length(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(cat(get_parameters(model), get_parameters(embed); dims=1), 0.05);\n",
    "\n",
    "indices = map(x->word2index[x], collect(raw))\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = trunc(Int,size(indices,1)/batch_size)\n",
    "trimmed_indices = indices[1:n_batches*batch_size]\n",
    "# batched_indices = reshape(trimmed_indices, (batch_size, n_batches))\n",
    "\n",
    "input_batched_indices = reshape(indices[1:n_batches*batch_size], (batch_size, n_batches))\n",
    "target_batched_indices = reshape(indices[2:n_batches*batch_size+1], (batch_size, n_batches))\n",
    "\n",
    "n_bptt = trunc(Int,((n_batches-1) / bptt))\n",
    "\n",
    "input_batches = permutedims(reshape(input_batched_indices[:,1:n_bptt*bptt], (n_bptt, bptt, batch_size)), (3,1,2))\n",
    "input_batches = reshape(input_batches, (32,16,195))\n",
    "\n",
    "target_batches = permutedims(reshape(target_batched_indices[:,1:n_bptt*bptt], (n_bptt, bptt, batch_size)), (3,1,2))\n",
    "target_batches = reshape(target_batches, (32,16,195));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(iterations=400;batch_size=32,bptt=16)\n",
    "    for iter=1:iterations\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = init_hidden(model, batch_size=batch_size)\n",
    "        for batch_i=1:size(input_batches,3)\n",
    "            loss = nothing\n",
    "            losses = []\n",
    "            hidden = Tensor(hidden.data, autograd=true)\n",
    "\n",
    "            for t=1:bptt\n",
    "                input = Tensor(input_batches[:,t,batch_i], autograd=true)\n",
    "                rnn_input = forward(embed, input)\n",
    "                output, hidden = forward(model, rnn_input, hidden)\n",
    "                \n",
    "                target = Tensor(target_batches[:,t,batch_i], autograd=true)\n",
    "                batch_loss = forward(criterion, output, target)\n",
    "                push!(losses, batch_loss)\n",
    "                \n",
    "                if t==1\n",
    "                    loss = batch_loss\n",
    "                else\n",
    "                    loss += batch_loss\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            backward(losses[end])\n",
    "            step(optim)\n",
    "            total_loss += losses[end].data\n",
    "            log = \"Iter: $(iter)\"\n",
    "            log *= \" - Batch $(batch_i)/$(size(input_batches,3))\"\n",
    "            log *= \" - Loss: $(exp(total_loss/batch_i))\"\n",
    "            if batch_i == 1\n",
    "                log*= \" - \" * replace(generate_sample(n=70, init_char='\\n'),'\\n' => \" \")\n",
    "            end\n",
    "            if ((batch_i-1) % 10 == 0) || batch_i == size(input_batches,3)\n",
    "                print(log,\"\\r\")\n",
    "            end\n",
    "        end\n",
    "        println()\n",
    "        optim.alpha *= 0.99\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_sample(;n=30,init_char=' ')\n",
    "    s = \"\"\n",
    "    hidden = init_hidden(model, batch_size=1)\n",
    "    input = Tensor([word2index[init_char]])\n",
    "    for i=1:n\n",
    "        rnn_input = forward(embed, input)\n",
    "        output, hidden = forward(model, rnn_input, hidden)\n",
    "        \n",
    "        output.data *= 2.0\n",
    "        temp_dist = softmax(output.data)\n",
    "        temp_dist ./= sum(temp_dist)\n",
    "        \n",
    "        m = argmax(temp_dist .>  rand()).I[1]\n",
    "        c = vocab[m]\n",
    "        input = Tensor([m])\n",
    "        s *= c\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 - Batch 195/195 - Loss: 226.646692627869939                                                                     \n",
      "Iter: 2 - Batch 195/195 - Loss: 27.035108320156823 he lo looooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Iter: 3 - Batch 195/195 - Loss: 19.796098831246475 ho thelonlonolololomanolononololomalomanomanomanolononononononond to t\n",
      "Iter: 4 - Batch 195/195 - Loss: 16.133433161753533ho lo therthelomer thelomand themand thelolomlolomand lomand lold loma\n",
      "Iter: 5 - Batch 195/195 - Loss: 14.134869831429294 ho lold would the theld and wold lomlom lomand would lomand lold lomth\n",
      "Iter: 6 - Batch 195/195 - Loss: 12.873099117955036 fo lo the theld lo theld and wo theld the theld and wold lomlold therd\n",
      "Iter: 7 - Batch 195/195 - Loss: 12.035796988313539 fo the lomer and lo the theld and wour the the lomer lold lomeld theld\n",
      "Iter: 8 - Batch 195/195 - Loss: 11.397050794965871 lo lomer the theld and and and and lomer tourd lomlomeld lomeld and an\n",
      "Iter: 9 - Batch 195/195 - Loss: 10.917590062850792 wo lomeld and and lold lomer lomeld the lomlour and and and and and an\n",
      "Iter: 10 - Batch 195/195 - Loss: 10.535734664420723 wo the to to tollour and and and lomy lomy wour and and and and and an\n",
      "Iter: 11 - Batch 195/195 - Loss: 10.212771217772076 wo lomy wour and and wour and and and and and and and and the lomy lom\n",
      "Iter: 12 - Batch 195/195 - Loss: 9.9275238189997174 wo lomy wourd, and and and and and and and and and and and and and and\n",
      "Iter: 13 - Batch 195/195 - Loss: 9.663612765921062 wo tlomy wour lomy wour and and and lomy wour and and and and and and \n",
      "Iter: 14 - Batch 195/195 - Loss: 9.413742902773185 wo lomy lomy wour and lold and and and and and and and and and and and\n",
      "Iter: 15 - Batch 195/195 - Loss: 9.174670121008544wo lomy wour lomy wour and and and and and and and and and and and and\n",
      "Iter: 16 - Batch 195/195 - Loss: 8.942474641991241 lold and and and and and and lomy wour homlour home, and and and loth \n",
      "Iter: 17 - Batch 195/195 - Loss: 8.709653810308716 wo tlomy lomy lold loth lold and and to to to to to to to to to to to \n",
      "Iter: 18 - Batch 195/195 - Loss: 8.488990358812297do to to to to to to to to to to to to to to to tlomy wold and and and\n",
      "Iter: 19 - Batch 195/195 - Loss: 8.281521766912979 doth lome, and and and and and and and and and and and and and and and\n",
      "Iter: 20 - Batch 195/195 - Loss: 8.081974946796157doth to to to to to to to to loth lold and and and and and and and and\n",
      "Iter: 21 - Batch 195/195 - Loss: 7.8874265422539075doth to to to to to to to to to to to to to to to to tlomy lome,ly wou\n",
      "Iter: 22 - Batch 195/195 - Loss: 7.6993861642925485loth to lome, Weard, and and and and and and and and and and and and a\n",
      "Iter: 23 - Batch 195/195 - Loss: 7.518744059829451 To longht wlomy and and and and and and and and and and and and and an\n",
      "Iter: 24 - Batch 195/195 - Loss: 7.3428855318024425 To my long and and and and and and and and and and and and and and and\n",
      "Iter: 25 - Batch 195/195 - Loss: 7.1694829634305415To my longht was and and and and and and and and and and and and and a\n",
      "Iter: 26 - Batch 195/195 - Loss: 6.9996818034234795 To loy and and and and and and and and and and and and and and and and\n",
      "Iter: 27 - Batch 195/195 - Loss: 6.8340798970768955Tother hlay.  DEANTING Shath to to to to to to to to to tlomy and and \n",
      "Iter: 28 - Batch 195/195 - Loss: 6.6704842707078136Tother hommy wollould and and and and and and and and and and and and \n",
      "Iter: 29 - Batch 195/195 - Loss: 6.507948152492856- Tother hom loy and and and and and and and and and and and to to to to\n",
      "Iter: 30 - Batch 195/195 - Loss: 6.3468868346287766 Tother hom the to to to to tlay, and and and and and and and and and a\n",
      "Iter: 31 - Batch 195/195 - Loss: 6.1881775531319565Tother hlay.  lume, and and and and and and and and and and and and an\n",
      "Iter: 32 - Batch 195/195 - Loss: 6.0330707444073345ot to to tle to to to to the was and and and and and and and and and \n",
      "Iter: 33 - Batch 195/195 - Loss: 5.8813445564597215Tot to to the my lold and and and and and and and and and and and and \n",
      "Iter: 34 - Batch 195/195 - Loss: 5.7316953343355215Tot to to the was and and and and and and and and and and and and and \n",
      "Iter: 35 - Batch 195/195 - Loss: 5.5833508254716355Tot to to the my longnther heard, Whould and and and and and and and a\n",
      "Iter: 36 - Batch 195/195 - Loss: 5.436308819129979 Notherly and and lose and whath to the lold and and and and and and an\n",
      "Iter: 37 - Batch 195/195 - Loss: 5.292130818124879 Nother'd and and whath to the longnthly and, and and lose longnthly an\n",
      "Iter: 38 - Batch 195/195 - Loss: 5.1519949486504295Nother'd and lose to the longnth, and and and whath whe mlancomy and a\n",
      "Iter: 39 - Batch 195/195 - Loss: 5.0160354539437425Nother'd lose and and losly and losly and and lose longnthly and, and \n",
      "Iter: 40 - Batch 195/195 - Loss: 4.8832887530699085Nother'd and and wlang honlame, and and lose longnth, and and and lose\n",
      "Iter: 41 - Batch 195/195 - Loss: 4.7528272662310125Nother'd and lold,ly and wlang honlly and whath when tle may hlent lon\n",
      "Iter: 42 - Batch 195/195 - Loss: 4.6240940556280635Nothle mllow, and and and and whath when the lameld be may heard,ly al\n",
      "Iter: 43 - Batch 195/195 - Loss: 4.4973126322420165Nother'd and lold, and whath whle month, and and whath whle monlang an\n",
      "Iter: 44 - Batch 195/195 - Loss: 4.3714124696174265Nolland and llow, and and lose and fory as and whathle monlangly as an\n",
      "Iter: 45 - Batch 195/195 - Loss: 4.2446080036614385Nother'd and whatherly as and when and fole to the fame, and and llow,\n",
      "Iter: 46 - Batch 195/195 - Loss: 4.1182316326762285Noh, and lold, and llow, and whatherd be may hims, and when tle longla\n",
      "Iter: 47 - Batch 195/195 - Loss: 3.9941717235915304 Noh, and whatherly all and flampt to the llow, and whatherd be may him\n",
      "Iter: 48 - Batch 195/195 - Loss: 3.8732178058515626Noh, and live and from from from from from from let, and whatherd be m\n",
      "Iter: 49 - Batch 195/195 - Loss: 3.7548050709210337 Noh, and whatherd be may hims, and live and flames, and whatherly all \n",
      "Iter: 50 - Batch 195/195 - Loss: 3.6379395804212766Noh, and whatherly as and was and live and from from from from from li\n",
      "Iter: 51 - Batch 195/195 - Loss: 3.5218245330306126 Tencomeld live and from live and from from live and frace, Well plames\n",
      "Iter: 52 - Batch 195/195 - Loss: 3.4078814596044023 loht mado and from from live and from live and live and from live and \n",
      "Iter: 53 - Batch 195/195 - Loss: 3.2969402466135666 Tencoment as and was and wlanly all whatherly and frle, and whatherd a\n",
      "Iter: 54 - Batch 195/195 - Loss: 3.1890521925712347Tencoment to the longer whather, and whather, and whather, and was and\n",
      "Iter: 55 - Batch 195/195 - Loss: 3.0847971026040666loht mallad: and was and was and wlange of and from the fainlaintagnne\n",
      "Iter: 56 - Batch 195/195 - Loss: 2.9844418997487635Tencoment allow, and was and was and was and was from the live and fro\n",
      "Iter: 57 - Batch 195/195 - Loss: 2.8887526989682915 Tencoment as was from the longer when the would be not, Whillain the w\n",
      "Iter: 58 - Batch 195/195 - Loss: 2.7959216079502642Noht I days and from the faintemn live and from the faintemn live and \n",
      "Iter: 59 - Batch 195/195 - Loss: 2.7064626953141482Tencoment as was from the longer when the would be not,ly, Wellow and \n",
      "Iter: 60 - Batch 195/195 - Loss: 2.6211777702924817 Noht I days and fory when the would be not, had fory when the would be\n",
      "Iter: 61 - Batch 195/195 - Loss: 2.5395046108818207Noht live duch thand, and was from the fainlaintar'd in come and was f\n",
      "Iter: 62 - Batch 195/195 - Loss: 2.4616735156662027Noht I days as the fare, and was from the fall pownly wother when the \n",
      "Iter: 63 - Batch 195/195 - Loss: 2.3868621717152645Noht live live as trued Mold be word as the fall pown.  Slitlly wother\n",
      "Iter: 64 - Batch 195/195 - Loss: 2.3134088985336767 Noht was I woso the wosill pownims, be not, Whice: I this was from the\n",
      "Iter: 65 - Batch 195/195 - Loss: 2.2427114665343453 Noht live live duch than my when when the wosing hims and was from liv\n",
      "Iter: 66 - Batch 195/195 - Loss: 2.1731875671529037Noht I days live duch than my when when thellowe.  DESALO: Whill as tr\n",
      "Iter: 67 - Batch 195/195 - Loss: 2.1026338381903594 Noht live duch lord, if come, my wrand, if not may your wother with Ra\n",
      "Iter: 68 - Batch 195/195 - Loss: 2.0363717390373597Noht madoman's hon my wrall pownims, be,--nots, Why fare, my wrall pow\n",
      "Iter: 69 - Batch 195/195 - Loss: 1.9764891086957197Noht in come in lill pownims, blother will your wother with wother wit\n",
      "Iter: 70 - Batch 195/195 - Loss: 1.9196998940988645 Noht liverver when mines hillas had fory wother, and was fory wother, \n",
      "Iter: 71 - Batch 195/195 - Loss: 1.8661014329431906 Noht live duch hompomy lord, if conselias will your wother with lord, \n",
      "Iter: 72 - Batch 195/195 - Loss: 1.8122765961846056 Noht I days lods, in hlew: There gence, and was fory wlange of your wo\n",
      "Iter: 73 - Batch 195/195 - Loss: 1.7644993211897222 Noht I lome, omylight inde, you wo live duch roy, and will your wother\n",
      "Iter: 74 - Batch 195/195 - Loss: 1.7152785325248794Noht I donge, my wrat As treeld blow may your lord, if conselves will \n",
      "Iter: 75 - Batch 195/195 - Loss: 1.6728755066212853 Noht I donge, my wrat Wollowe As the loss they have may your what Ton \n",
      "Iter: 76 - Batch 195/195 - Loss: 1.6325080550798146Noht I dong fory when I woso the wosh wotly wother will your wother to\n",
      "Iter: 77 - Batch 195/195 - Loss: 1.5910916255321084Noh, I'll denceas there be not, hil let may your grace'd in clos; and \n",
      "Iter: 78 - Batch 195/195 - Loss: 1.5593431503496036 Noh all they they they they they they they they they they they thele l\n",
      "Iter: 79 - Batch 195/195 - Loss: 1.5401935299843428 Noh a donds in the wosh throw not mayw and ey live in put fit there be\n",
      "Iter: 80 - Batch 195/195 - Loss: 1.5266020600440733 Noh all, in the wosh threwollowe As the for our hil act be hillain the\n",
      "Iter: 81 - Batch 195/195 - Loss: 1.4881875120894446 Noh a donds in the wosh threwoty, nonday.  POlos; Will I not, but my w\n",
      "Iter: 82 - Batch 195/195 - Loss: 1.4616571425959308 loh you wose the live dly diddaling in put fit the loss they they they\n",
      "Iter: 83 - Batch 195/195 - Loss: 1.4205341654120138 loh live des is conqure of your grace'd in conset giververy wiff ey we\n",
      "Iter: 84 - Batch 195/195 - Loss: 1.3827180251742037 Npeakiour hils.  PO: Whou would but I wose live did dllow in chase you\n",
      "Iter: 85 - Batch 195/195 - Loss: 1.3524764115887473 Npeakiour hilsell you live in put fither the itsee, lines in live in p\n",
      "Iter: 86 - Batch 195/195 - Loss: 1.3304464447396167 Npt but I would but I woll affeach, But I lord ass in the live delie, \n",
      "Iter: 87 - Batch 195/195 - Loss: 1.3053566556278857 Npeakiolty of fie, your grall, in the live des is conty, thyself as tr\n",
      "Iter: 88 - Batch 195/195 - Loss: 1.2814152677972763 Npeakiour hims, lody.  Speakiour hims, but I dladly wiff ey wear face,\n",
      "Iter: 89 - Batch 195/195 - Loss: 1.2668775853978452Npeakioltion, in chasety Tlese subures: There guill deads are, my wren\n",
      "Iter: 90 - Batch 195/195 - Loss: 1.3333111502100865 Npeakio, put fithrly threword. Ghak: id and ey if live in pon the woul\n",
      "Iter: 91 - Batch 195/195 - Loss: 1.3307150707774666 Nety There live in pon the would be not, but may your grall, in this w\n",
      "Iter: 92 - Batch 195/195 - Loss: 1.3251629372393946 Npeak: Plody.  Speakiour hims, but I wosh threword.  DESlOUN: lody.  S\n",
      "Iter: 93 - Batch 195/195 - Loss: 1.2838841547329465 Nptly what: The itserlightly lotin.  lOUTEWALVS: Wily'd and fare, oleo\n",
      "Iter: 94 - Batch 195/195 - Loss: 1.2637343353412844 Thtid furth. shou gnother, and what: There liververe you we sen from t\n",
      "Iter: 95 - Batch 195/195 - Loss: 1.2574497825622266 Twtly hatrs for the live and ey if not beget lett wifl what warw; I do\n",
      "Iter: 96 - Batch 195/195 - Loss: 1.2217390949151458Twterd as trle drell let there wort as is had notin.  LOUNEN: Will not\n",
      "Iter: 97 - Batch 195/195 - Loss: 1.1951411703452886Twterd: I dayw and fellowe in conqure of livernnother, Thou gord and f\n",
      "Iter: 98 - Batch 195/195 - Loss: 1.1785547948437948 Twterdly hatrserle sat fithrewn. Behose in live in put the word poing \n",
      "Iter: 99 - Batch 195/195 - Loss: 1.2003122343494743 Twle liververe wife and embllow herong live dest a mide.  PANTESTELUS:\n",
      "Iter: 100 - Batch 195/195 - Loss: 1.1738815135024936wtly hather to Taffert but I would but I would but I would be had Sha\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twtit hewnow livelyle go.\n",
      "\n",
      "PORIll larghing so my sare, youl he bed the guill let thele live live loldmers, beslall in the kll with R lever's thee, marringt to the my lord.\n",
      "lood live live and but the knowly wlanly weals.\n",
      "\n",
      "lOUll let thele live live glad deligh.\n",
      "\n",
      "Slith luest whalff thee\n",
      "Ande in nondodinly are, ther llanknow your llall let thee.\n",
      "\n",
      "DUKE Vlother,ly banly live are, thelf llenly are, lewll was trueld blother let lett whle glowl let the mlet give in from the guill let thele live dest and fare, oll and but the lows lill'd in the know you fromy lord as live all this beddly lilloll let thee\n",
      "Ande in nongur live live live lowes is the pain.\n",
      "\n",
      "Slill plalll he dllow youl with thy lest lodstly.\n",
      "\n",
      "GLEll pow thysle all let thee\n",
      "And live dest allly linesling boyour be dlew all thruntly bandly dlale of your lold be live glall see hap all this bedly linesling blow lord, forserlell let thell you live dest a mlell in the kll with the know you let the may you may you may youl he live are, ther light thee\n",
      "All lut the may let madly a dould lient what\n",
      "To glich not lordly liness.\n",
      "\n",
      "lunger thle gook as live live menty lett wllow your llain thle lill'd live live live live deslignllowe the parado, you well dongurldolly desing he dest a my,\n",
      "But maywrell you a sleme, omy famestrealmosell you lly a dould dinder with the know you live did domldlish, look as live dest are, mlet you the live gence trunkly larldou slame, I'll if live dest a mylight thee\n",
      "At well dongurldor live beholt lunger lilth, leld embriend, if can rest are, let yle guill lell you live dest a my lord as tree hap sen fromy lord live be name, om tole, you livell let lett wllowly dalightly pow wlenly wll with the know ylines:\n",
      "The llow all lllighll live dest all.\n",
      "\n",
      "PORK:\n",
      "But slewll was hath dlung be have will let thele live live lowd. leans, if live dest all.\n",
      "\n",
      "lOh; I llolld had\n",
      "And fallfleat you the live llow hut the not bell llow your llain the Dis herlioltit heldly lurth.\n",
      "\n",
      "lOng blow all luld blother let the losl in a see th"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
