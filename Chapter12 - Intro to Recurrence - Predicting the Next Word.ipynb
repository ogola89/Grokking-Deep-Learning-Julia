{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download & Preprocess the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "function pretty_print_review_and_label(i)\n",
    "    println(labels[i] * \"\\t\\t\" * reviews[i][:80]*\"...\")\n",
    "end\n",
    "g = open(\"reviews.txt\", \"r\");\n",
    "reviews = map(x -> x[1:end-1], readlines(g))\n",
    "close(g)\n",
    "\n",
    "g = open(\"labels.txt\", \"r\")\n",
    "labels = map(x -> x[1:end-1], readlines(g))\n",
    "close(g)\n",
    "\n",
    "# Preprocess dataset:\n",
    "\n",
    "f = open(\"reviews.txt\")\n",
    "raw_reviews = readlines(f)\n",
    "close(f)\n",
    "\n",
    "f = open(\"labels.txt\")\n",
    "raw_labels = readlines(f)\n",
    "close(f)\n",
    "\n",
    "tokens = collect(map(x -> Set(split(x, \" \")), raw_reviews))\n",
    "\n",
    "vocab = Set()\n",
    "for sent in tokens\n",
    "    for word in sent\n",
    "        if length(word)>0\n",
    "            push!(vocab, word)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "vocab = collect(vocab)\n",
    "\n",
    "word2index = Dict()\n",
    "for (i,word) in enumerate(vocab)\n",
    "    word2index[word] = i\n",
    "end\n",
    "\n",
    "input_dataset = []\n",
    "for sent in tokens\n",
    "    sent_indices = []\n",
    "    for word in sent\n",
    "        try\n",
    "            push!(sent_indices, word2index[word])\n",
    "        catch\n",
    "            nothing\n",
    "        end\n",
    "    end\n",
    "    push!(input_dataset, Set(sent_indices))\n",
    "end\n",
    "\n",
    "target_dataset = []\n",
    "for label in raw_labels\n",
    "    if label == \"positive\"\n",
    "        push!(target_dataset, 1)\n",
    "    else\n",
    "        push!(target_dataset, 0)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Surprising Power of Averaged Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " \"just read the original story which is written by p\"\n",
       " \"more eeriness and dark secrets released in the fin\"\n",
       " \"an enjoyable game  which offers fun and easy game \"\n",
       " \"i have seen a lot of movies . . . this is the firs\"\n",
       " \"cheap  amateurish  unimaginative  exploitative . .\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics:mean\n",
    "norms = sum(weights_0_1 .* weights_0_1;dims=1)\n",
    "normed_weights = weights_0_1 .* norms\n",
    "\n",
    "function make_sent_vect(words)\n",
    "    indices = map(x -> word2index[x],collect(filter(x -> haskey(word2index,x),words)))\n",
    "    return mean(normed_weights[:,indices];dims=2)\n",
    "end\n",
    "\n",
    "reviews2vectors = []\n",
    "for review in tokens # tokenized reviews\n",
    "    push!(reviews2vectors, make_sent_vect(review))\n",
    "end\n",
    "\n",
    "reviews2vectors = reduce(hcat, reviews2vectors)\n",
    "function most_similar_reviews(review)\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Dict()\n",
    "    for (i,val) in enumerate(v' * reviews2vectors)\n",
    "        scores[i] = val\n",
    "    end\n",
    "    most_similar = []\n",
    "    \n",
    "    scores = sort(collect(scores), by = x -> x[2])\n",
    "    for i in scores[1:5]\n",
    "        idx,score = i\n",
    "        push!(most_similar,raw_reviews[idx][1:50])\n",
    "    end\n",
    "    return most_similar\n",
    "end\n",
    "\n",
    "most_similar_reviews([\"boring\",\"awful\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices that Change Absolutely Nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—3 Array{Bool,2}:\n",
       " 1  0  0\n",
       " 0  1  0\n",
       " 0  0  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra: I\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [0.1,0.2,0.3]\n",
    "c = [-1,-0.5,0]\n",
    "d = [0,0,0]\n",
    "\n",
    "Identity = Matrix(I,(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[0.1, 0.2, 0.3]\n",
      "[-1.0, -0.5, 0.0]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "println(Identity * a)\n",
    "println(Identity * b)\n",
    "println(Identity * c)\n",
    "println(Identity * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 15, 17]\n",
      "[13, 15, 17]\n"
     ]
    }
   ],
   "source": [
    "this = [2,4,6]\n",
    "movie = [10,10,10]\n",
    "rocks = [1,1,1]\n",
    "\n",
    "println(this + movie + rocks)\n",
    "println(Identity * this + Identity * movie + Identity * rocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function softmax(x)\n",
    "    temp = exp.(x)\n",
    "    return temp ./ sum(temp;dims=1)\n",
    "end\n",
    "\n",
    "word_vects = Dict()\n",
    "word_vects[\"yankees\"] = [0.,0.,0.]\n",
    "word_vects[\"bears\"] = [0.,0.,0.]\n",
    "word_vects[\"braves\"] = [0.,0.,0.]\n",
    "word_vects[\"red\"] = [0.,0.,0.]\n",
    "word_vects[\"socks\"] = [0.,0.,0.]\n",
    "word_vects[\"lose\"] = [0.,0.,0.]\n",
    "word_vects[\"defeat\"] = [0.,0.,0.]\n",
    "word_vects[\"beat\"] = [0.,0.,0.]\n",
    "word_vects[\"tie\"] = [0.,0.,0.]\n",
    "\n",
    "sent2output = rand(length(word_vects),3)\n",
    "\n",
    "Identity = 1.0 .* Matrix(I,(3,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111]\n"
     ]
    }
   ],
   "source": [
    "layer_0 = word_vects[\"red\"]\n",
    "layer_1 = Identity * layer_0 + word_vects[\"socks\"]\n",
    "layer_2 = Identity * layer_1 + word_vects[\"defeat\"]\n",
    "\n",
    "pred = softmax(sent2output * layer_2)\n",
    "println(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we Backpropagate into this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1,0,0,0,0,0,0,0,0] # target one-hot vector for \"yankees\"\n",
    "\n",
    "pred_delta = pred .- y\n",
    "layer_2_delta = sent2output' * pred_delta\n",
    "defeat_delta = layer_2_delta .* 1 # can ignore the \"1\" like prev. chapter\n",
    "layer_1_delta = Identity * layer_2_delta\n",
    "socks_delta = layer_1_delta * 1 # again... can ignore the \"1\"\n",
    "layer_0_delta = Identity * layer_1_delta\n",
    "alpha = 0.01\n",
    "word_vects[\"red\"] .-= layer_0_delta .* alpha\n",
    "word_vects[\"socks\"] .-= socks_delta .* alpha\n",
    "word_vects[\"defeat\"] .-= defeat_delta .* alpha\n",
    "Identity .-= layer_0' * layer_1_delta .* alpha\n",
    "Identity .-= layer_1' * layer_2_delta .* alpha\n",
    "sent2output .-= pred_delta * layer_2' .* alpha;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = readlines(\"tasksv11/en/qa1_single-supporting-fact_train.txt\")\n",
    "close(f)\n",
    "\n",
    "tokens = []\n",
    "for line in raw[1:1000]\n",
    "    push!(tokens, split(lowercase(line),\" \")[2:end])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Set()\n",
    "for sent in tokens\n",
    "    for word in sent\n",
    "        if length(word)>0\n",
    "            push!(vocab, word)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "vocab = collect(vocab)\n",
    "\n",
    "word2index = Dict()\n",
    "for (i,word) in enumerate(vocab)\n",
    "    word2index[word] = i\n",
    "end\n",
    "\n",
    "function words2indices(sentence)\n",
    "    idx =[]\n",
    "    for word in sentence\n",
    "        push!(idx, word2index[word])\n",
    "    end\n",
    "    return idx\n",
    "end\n",
    "\n",
    "function softmax(x)\n",
    "    e_x = exp.(x .- maximum(x))\n",
    "    return e_x ./ sum(e_x ;dims=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random: seed!\n",
    "seed!(1)\n",
    "embed_size = 10\n",
    "\n",
    "# word embeddings\n",
    "embed = (rand(embed_size, length(vocab)) .- 0.5) .* 0.1\n",
    "\n",
    "# embedding -> embedding (initially the identity matrix)\n",
    "recurrent = 1.0 .* Matrix(I,(embed_size, embed_size));\n",
    "\n",
    "# sentence embedding for empty sentence\n",
    "start = zeros(embed_size,1)\n",
    "\n",
    "# embedding -> output weights\n",
    "decoder = (rand(length(vocab), embed_size) .- 0.5) .* 0.1\n",
    "\n",
    "# one hot lookups (for loss function)\n",
    "one_hot = 1.0 .* Matrix(I,(length(vocab), length(vocab)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(sent)\n",
    "    layers = []\n",
    "    layer = Dict()\n",
    "    layer[\"hidden\"] = start\n",
    "    push!(layers, layer)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # forward propagate\n",
    "    preds = []\n",
    "    for target_i=1:length(sent)\n",
    "        \n",
    "        layer = Dict()\n",
    "        # try to predict the next term\n",
    "\n",
    "        layer[\"pred\"] = softmax(layers[end][\"hidden\"]' * decoder')  ##why transpose?\n",
    "        \n",
    "        loss += -log(layer[\"pred\"][sent[target_i]])\n",
    "        \n",
    "        # generate the next hidden state\n",
    "        \n",
    "        layer[\"hidden\"] = recurrent * layers[end][\"hidden\"] .+ embed[:,sent[target_i]]\n",
    "        push!(layers, layer)\n",
    "    end\n",
    "    return layers, loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Update with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 82.12804928343257\n",
      "Perplexity: 81.9288000682471\n",
      "Perplexity: 81.65027823771905\n",
      "Perplexity: 81.13600268626917\n",
      "Perplexity: 80.05772120028277\n",
      "Perplexity: 77.52590840620103\n",
      "Perplexity: 70.03713721328808\n",
      "Perplexity: 41.84456883637827\n",
      "Perplexity: 25.843694020403923\n",
      "Perplexity: 19.57887591173625\n",
      "Perplexity: 18.07436760577069\n",
      "Perplexity: 16.47333734739069\n",
      "Perplexity: 14.158483856457426\n",
      "Perplexity: 11.069994257133942\n",
      "Perplexity: 8.408635577993751\n",
      "Perplexity: 7.179980870657079\n",
      "Perplexity: 6.520578072729803\n",
      "Perplexity: 6.156516553473082\n",
      "Perplexity: 5.885074372332072\n",
      "Perplexity: 5.594645689787941\n",
      "Perplexity: 5.337171014994522\n",
      "Perplexity: 5.147295077016582\n",
      "Perplexity: 5.010296680607723\n",
      "Perplexity: 4.910781155742089\n",
      "Perplexity: 4.840917261834581\n",
      "Perplexity: 4.796960480945621\n",
      "Perplexity: 4.7757649057505756\n",
      "Perplexity: 4.7589698400308365\n",
      "Perplexity: 4.660993584220425\n",
      "Perplexity: 4.401230989635085\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "\n",
    "for iter=1:30000\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[(iter % length(tokens))+1][2:end])\n",
    "    \n",
    "    layers,loss = predict(sent)\n",
    "\n",
    "    # back propagate\n",
    "    for layer_idx in reverse(1:length(layers))\n",
    "        \n",
    "        layer = layers[layer_idx]\n",
    "          \n",
    "        if (layer_idx > 1)\n",
    "            target = sent[layer_idx-1]\n",
    "            \n",
    "            layer[\"output_delta\"] = layer[\"pred\"] - reshape(one_hot[:,target], (1,:))\n",
    "            new_hidden_delta =  decoder' * layer[\"output_delta\"]'\n",
    "            \n",
    "            # if the last layer - don't pull from a \n",
    "            # later one becasue it doesn't exist\n",
    "            if(layer_idx == length(layers))\n",
    "                layer[\"hidden_delta\"] = new_hidden_delta\n",
    "            else\n",
    "                layer[\"hidden_delta\"] = new_hidden_delta + recurrent' * layers[layer_idx+1][\"hidden_delta\"] \n",
    "            end\n",
    "        else\n",
    "            layer[\"hidden_delta\"] = recurrent' * layers[layer_idx+1][\"hidden_delta\"]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    start -= layers[1][\"hidden_delta\"] .* (alpha / length(sent))\n",
    "    \n",
    "    for (layer_idx,layer) in enumerate(layers[2:end])\n",
    "        decoder .-= layer[\"output_delta\"]' * layers[layer_idx][\"hidden\"]' .* (alpha / length(sent))\n",
    "\n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[:,embed_idx] .-= dropdims(layers[layer_idx][\"hidden_delta\"] .* (alpha / length(sent));dims=2)\n",
    "        recurrent -= layer[\"hidden_delta\"] * layers[layer_idx][\"hidden\"]' .* (alpha / length(sent))\n",
    "    end\n",
    "    if((iter-1) % 1000 == 0)\n",
    "        \n",
    "        println(\"Perplexity: $(exp(loss/length(sent)))\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubString{String}[\"sandra\", \"moved\", \"to\", \"the\", \"garden.\"]\n",
      "Prev Input: sandra  True: moved  Pred: is\n",
      "Prev Input: moved  True: to  Pred: to\n",
      "Prev Input: to  True: the  Pred: the\n",
      "Prev Input: the  True: garden.  Pred: bathroom.\n"
     ]
    }
   ],
   "source": [
    "sent_index = 5\n",
    "\n",
    "l,_ = predict(words2indices(tokens[sent_index]))\n",
    "\n",
    "println(tokens[sent_index])\n",
    "\n",
    "for (i, each_layer) in enumerate(l[2:end-1])\n",
    "    input = tokens[sent_index][i]\n",
    "    True = tokens[sent_index][i+1]\n",
    "    pred = vocab[argmax(each_layer[\"pred\"][1,:])]\n",
    "    println(\"Prev Input: $(input)  True: $(True)  Pred: $(pred)\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
